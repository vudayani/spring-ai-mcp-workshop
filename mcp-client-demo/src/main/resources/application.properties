spring.application.name=mcp-client-demo

# ===================================================================
# LLM Provider Configuration
# ===================================================================
#
# Uncomment the properties for the LLM provider you want to use.
# Make sure you have set the corresponding API key in your environment variables.

# ---------------- OpenAI ----------------
spring.ai.openai.api-key=${OPENAI_API_KEY}
spring.ai.openai.chat.options.model=gpt-4o

# ---------------- Anthropic ----------------
#spring.ai.anthropic.api-key=${SPRING_AI_ANTHROPIC_API_KEY}
#spring.ai.anthropic.chat.options.model=claude-3-sonnet-20240229

# ---------------- Ollama ----------------
#spring.ai.ollama.base-url=http://localhost:11434
#spring.ai.ollama.chat.options.model=llama3

# MCP servers config
spring.ai.mcp.client.stdio.servers-configuration=classpath:/mcp-servers-config.json

# On-Call MCP server config in HTTP mode
# spring.ai.mcp.client.streamable-http.connections.on-call.url=http://localhost:8081/mcp
